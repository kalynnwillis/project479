---
title: "Player Impact Analysis via Win Probability RAPM: NCAA Men's Basketball 2018-2024"
author: "NCAA RAPM Analysis Team"
date: "October 6, 2025"
format:
  html:
    toc: true
    toc-depth: 3
    number-sections: true
    code-fold: show
    code-tools: true
    theme: cosmo
    embed-resources: true
---

# Executive Summary

This analysis quantifies individual player impact in NCAA Division I men's basketball using a win probability framework applied to seven seasons of ESPN play-by-play data (2018-2024). Our goal is to measure how much each player moves the needle on their team's chance of winning, accounting for game context, teammate quality, and opponent strength.

**Approach:** We implement a two-stage methodology. First, we build calibrated win probability models from game state features (score differential, time remaining, possession, leverage). We test four model types—logistic regression, gradient boosting machines, random forest, and XGBoost—and select the best performer based on Brier score and calibration quality rather than discrimination alone. Second, we compute change in win probability (ΔWP) per minute for each stint and estimate Regularized Adjusted Plus-Minus (RAPM) using ridge, lasso, and elastic net regression with team, conference, and season fixed effects.

**Main Results:** Logistic regression emerged as the best win probability model (test AUC = 0.858, Brier = 0.151) with near-perfect calibration (slope = 1.05). Despite lower AUC, it outperformed XGBoost on calibration by a wide margin, demonstrating that well-calibrated probabilities matter more than maximum discrimination when the goal is accurate ΔWP calculation.

For player evaluation, we analyzed 2,316 players meeting minimum thresholds (≥2,000 minutes, ≥50 games). Top performers include Isiaih Mosley (Ridge RAPM = 0.131 per 40 min), Sir'Jabari Rice (0.108), and Gavin Kensmil (0.101). These values represent substantial impact: Mosley's 0.131 translates to approximately a 13 percentage point win probability advantage over 40 minutes. The distribution of RAPM values is roughly normal and centered near zero, with a shrinkage factor of 0.15 (Ridge SD relative to unregularized baseline), reflecting strong but appropriate regularization given noisy stint-level data.

Conference fixed effects revealed modest but real systematic differences (±2% WP per 40 min), suggesting that league environment, coaching quality, or system effects contribute to performance beyond individual player talent.

**Critical Implementation Detail:** Player identity tracking by unique athlete ID (rather than display name) proved essential—we identified 1,411 display names mapping to multiple different athletes across our dataset. Aggregating by name would have created phantom "super-players" and invalidated rankings.

**Limitations:** ESPN data lacks substitution events, forcing us to use starter-based, half-level stints as a proxy for true lineup shifts. This makes player-level RAPM exploratory rather than definitive, though team-level estimates remain robust. Additionally, we cannot capture within-half lineup changes or bench player contributions systematically.

**Next Steps:** Acquiring substitution data would transform this analysis, enabling possession-level lineup tracking. Further improvements include Bayesian hierarchical priors for position-specific shrinkage, opponent lineup interactions for matchup effects, and rigorous cross-validation across seasons and conferences for robustness testing. The framework is production-ready pending higher-resolution lineup data.

# Data & Methods

## Data Sources and Scope

```{r setup, message=FALSE, warning=FALSE}
library(tidyverse)
library(readr)
library(knitr)
library(kableExtra)

# Load saved results
wp_eval <- read_csv("tables/wp_model_evaluation.csv", show_col_types = FALSE)
rapm_main <- read_csv("tables/rapm_rankings.csv", show_col_types = FALSE)
rapm_1500 <- read_csv("tables/rapm_rankings_1500min.csv", show_col_types = FALSE)
rapm_2500 <- read_csv("tables/rapm_rankings_2500min.csv", show_col_types = FALSE)
top_bottom <- read_csv("tables/top_bottom_players.csv", show_col_types = FALSE)
conf_eff <- read_csv("tables/conference_effects.csv", show_col_types = FALSE)
season_eff <- read_csv("tables/season_effects.csv", show_col_types = FALSE)
rapm_stats <- read_csv("tables/rapm_summary_stats.csv", show_col_types = FALSE)

# Load full objects
wp_models <- readRDS("data/interim/wp_models.rds")
rapm_results <- readRDS("data/processed/rapm_results.rds")
player_profiles <- readRDS("data/interim/player_profiles.rds")
```

We analyze **seven seasons** of NCAA Division I men's basketball (2018-2024) using play-by-play and box score data from ESPN via the `hoopR` R package. The dataset comprises:

- **38,625 games** across all NCAA D-I teams
- **12.48 million play-by-play events** with game state information
- **808,826 player-game box score records** 
- **35,457 unique players** identified by ESPN's `athlete_id`

**Critical data engineering:** We track players by unique `athlete_id` rather than display name. In our dataset, **1,411 display names map to multiple different athletes** (e.g., "Jalen Johnson" refers to 18 distinct individuals). Using display names for aggregation would create phantom "super-players" with inflated statistics and invalid rankings.

After filtering to games with complete starting lineup data (99.4% of games), we construct **37,974 half-level stints** covering **9,917 unique players** who appeared as starters.

## Win Probability Modeling

### Features and Model Specifications

We construct win probability models using the following features derived from play-by-play game state:

- **Score differential** (home score - away score)
- **Time remaining** (in minutes) and elapsed time
- **Interactions:** score_diff × time_remaining, score_diff²
- **Possession indicator** (when available; defaults to 0.5 if missing)
- **Half indicators** (first half, second half)
- **Clutch indicator** (time < 5 min AND |score_diff| < 10)

We train and evaluate four model types:

1. **Logistic Regression** (GLM with binomial family)
2. **Gradient Boosting Machine** (GBM, 300 trees, depth 3)
3. **Random Forest** (ranger implementation, 300 trees)
4. **XGBoost** (200 rounds, depth 4)

### Model Selection Criteria

**Critical insight:** We prioritize **Brier score and calibration quality** over AUC. Why? We use predicted probabilities to compute ΔWP—if a model systematically overestimates or underestimates probabilities (poor calibration), that bias propagates through the entire RAPM estimation.

**Calibration assessment:** We fit `glm(actual ~ qlogis(pred), family=binomial)` on test predictions. Perfect calibration yields:

- **Intercept ≈ 0** (no systematic bias)
- **Slope ≈ 1** (correct spread/confidence)

```{r wp-model-results}
# Test set performance
wp_eval %>%
  filter(Dataset == "Test") %>%
  select(Model, AUC, Brier_Score, Calib_Intercept, Calib_Slope) %>%
  kable(digits = 3, 
        col.names = c("Model", "AUC", "Brier Score", "Calibration Intercept", "Calibration Slope"),
        caption = "Win Probability Model Performance (Test Set)") %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE)
```

**Selected model:** `r wp_models$best_model_name` achieves the best Brier score (`r sprintf("%.4f", min(wp_eval$Brier_Score[wp_eval$Dataset == "Test"]))`) with calibration slope near 1.0, despite slightly lower AUC than XGBoost.

## Stint Construction and Response Variable

### Lineup Tracking Constraints

**Data limitation:** ESPN play-by-play does not include substitution events. We cannot observe exact lineup changes during games.

**Solution:** We use **starter-based, half-level stints** as a defensible proxy:

1. Identify starting lineups from box score `starter` flag
2. Assume starters play entire half together (no substitutions tracked)
3. Compute aggregate ΔWP for each game-half stint
4. Weight by duration × leverage (close/clutch games upweighted)

This approach yields **37,974 valid 5v5 stints** with median duration ~20 minutes (one half).

### Response Variable: ΔWP per Minute

For each stint, we compute:

$$\Delta WP_{stint} = WP_{end} - WP_{start}$$

We normalize to per-minute scale and apply outlier guards:

```r
y = stint$wp_change / max(stint$duration, 60) * 60  # per-minute, min duration 60s
y = pmin(pmax(y, -0.02), 0.02)  # cap at ±2% WP per minute
```

### Weighting and Time Decay

**Duration weighting:** $w_i = \sqrt{duration_i}$ (longer stints more reliable)

**Leverage weighting:** 
- Close + clutch (|diff| ≤ 10, time < 5 min): weight × 2.0
- Close only: weight × 1.5  
- Blowout (|diff| > 20): weight × 0.5
- Normal: weight × 1.0

**Time decay:** Recent seasons weighted higher with exponential decay (half-life = 1 season):
$$w_{season} = \exp(-\lambda \cdot (current\_season - stint\_season))$$

Weights are renormalized after decay to maintain total effective sample size.

## RAPM Estimation

### Design Matrix

We construct a **sparse design matrix** where each column represents a unique player (by `athlete_id`):

- Home starters: coefficient = +1
- Away starters: coefficient = -1
- Each stint contributes 10 non-zero entries (5 home, 5 away)

We augment with **fixed effects**:

- **Team effects** (captures team-level factors beyond player quality)
- **Conference effects** (league environment, coaching systems)
- **Season effects** (accounts for rule changes, era differences)

Final matrix dimensions: 37,974 stints × (9,917 players + teams + conferences + seasons)

### Regularization and Cross-Validation

We estimate using `glmnet` with weighted observations:

1. **Ridge regression** (α = 0): L2 penalty, shrinks all coefficients toward zero
2. **Lasso regression** (α = 1): L1 penalty, performs variable selection  
3. **Elastic Net** (α = 0.5): L1+L2 combination
4. **Baseline OLS** (λ ≈ 0): Unregularized sanity check

**Hyperparameter selection:** 5-fold cross-validation, select `lambda.1se` (more aggressive shrinkage than `lambda.min` for stability).

**Parallel processing:** 9 cores for faster CV evaluation.

### Validation Checks

**Sanity checks:**
- Mean RAPM ≈ 0 (verified: `r sprintf("%.6f", rapm_results$sanity_checks$rapm_mean)`)
- Correlation with minutes ≈ 0 (verified: `r sprintf("%.3f", rapm_results$sanity_checks$minutes_cor)`)

**Out-of-sample validation:** Game-wise leave-one-out MSE on 50 held-out games:
- Ridge OOS MSE: `r sprintf("%.6f", rapm_results$oos_validation$ridge_mse)`
- Baseline OOS MSE: `r sprintf("%.6f", rapm_results$oos_validation$baseline_mse)`  
- Improvement: `r sprintf("%.1f%%", 100 * (1 - rapm_results$oos_validation$ridge_mse / rapm_results$oos_validation$baseline_mse))`

# Results

## Win Probability Model Performance

### Calibration Quality

![Win Probability Model Calibration Curves](figs/wp_calibration.png){fig-alt="Calibration plots for all four WP models showing predicted vs observed win rates" width="85%"}

**Figure 1:** Calibration curves for all candidate models. Points near the diagonal (y=x) indicate good calibration. Logistic (purple) tracks the diagonal most closely, while XGBoost (yellow) shows systematic overconfidence at extreme probabilities.

![Best Model Calibration Detail](figs/wp_calibration_best.png){fig-alt="Detailed calibration plot for the selected Logistic model" width="70%"}

**Figure 2:** Detailed calibration for the selected Logistic model. Near-perfect alignment with the diagonal across all probability ranges (slope = 1.05, intercept = -0.23).

### Discrimination Performance

![Model AUC Comparison](figs/wp_model_performance.png){fig-alt="Bar chart comparing train and test AUC across all models" width="70%"}

**Figure 3:** AUC comparison across models. All models achieve similar discrimination (AUC 0.85-0.86), with minimal overfitting (test AUC close to train AUC).

## Player RAPM Rankings

### Top Performers

```{r top-20-players}
rapm_main %>%
  arrange(desc(ridge_rapm)) %>%
  select(player, ridge_per40, games_played, total_minutes, ridge_percentile) %>%
  slice_head(n = 20) %>%
  kable(digits = 3,
        col.names = c("Player", "Ridge RAPM (per 40 min)", "Games", "Total Minutes", "Percentile"),
        caption = "Top 20 Players by Ridge RAPM (≥2000 min, ≥50 games)") %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE)
```

**Interpretation:** Isiaih Mosley's RAPM of 0.131 per 40 minutes translates to approximately **13 percentage points** of win probability advantage in a full game (assuming ~80 possessions). This represents elite impact—99.9th percentile among qualified players.

![Top 30 Players Visualization](figs/top_players_rapm.png){fig-alt="Bar chart of top 30 players by Ridge RAPM" width="90%"}

**Figure 4:** Visual representation of top players. The rapid drop-off after the top 3-5 players reflects appropriate shrinkage.

### Distribution and Regularization

![RAPM Distribution](figs/rapm_distribution.png){fig-alt="Histogram of Ridge RAPM values showing approximately normal distribution" width="70%"}

**Figure 5:** Distribution of Ridge RAPM values. Approximately normal, centered near zero as expected. The symmetric spread indicates the model captures both positive and negative outliers.

```{r shrinkage-analysis}
# Compute shrinkage factor
baseline_sd <- sd(rapm_main$baseline_per40, na.rm = TRUE)
ridge_sd <- sd(rapm_main$ridge_per40, na.rm = TRUE)
shrinkage_factor <- ridge_sd / baseline_sd

tibble(
  Metric = c("Baseline APM SD", "Ridge RAPM SD", "Shrinkage Factor"),
  Value = c(baseline_sd, ridge_sd, shrinkage_factor)
) %>%
  kable(digits = 4,
        caption = "Regularization Shrinkage Summary") %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE)
```

**Shrinkage factor = `r sprintf("%.3f", shrinkage_factor)`:** Ridge reduces standard deviation to ~15% of baseline estimates, reflecting strong regularization appropriate for noisy stint-level data.

![Ridge vs Lasso Comparison](figs/ridge_vs_lasso.png){fig-alt="Scatter plot comparing Ridge and Lasso RAPM estimates" width="70%"}

**Figure 6:** Ridge vs Lasso comparison. Lasso shrinks most coefficients to exactly zero (horizontal line), while Ridge preserves more variation with proportional shrinkage.

### Relationships with Covariates

![RAPM vs Average Minutes](figs/rapm_vs_minutes.png){fig-alt="Scatter plot of RAPM vs average minutes played with loess smooth" width="70%"}

**Figure 7:** RAPM vs minutes played. The flat loess curve (correlation ≈ `r sprintf("%.3f", rapm_results$sanity_checks$minutes_cor)`) confirms that our regularization successfully removes bias toward high-minute players.

![RAPM vs Shooting Efficiency](figs/rapm_vs_shooting.png){fig-alt="Scatter plot of RAPM vs free throw percentage, colored by 3-point percentage" width="70%"}

**Figure 8:** RAPM vs shooting efficiency. Positive relationship between free throw percentage and RAPM, with 3-point shooting (color) showing additional signal. Elite shooters cluster in the upper right.

![RAPM by Position](figs/rapm_by_position.png){fig-alt="Box plots comparing RAPM distributions across positions (Guard, Forward, Center)" width="70%"}

**Figure 9:** RAPM distribution by position. Guards show wider spread with both high-impact playmakers and negative-impact players. Centers show tighter clustering, possibly reflecting more consistent but limited roles.

![Elite Shooters](figs/elite_shooters_rapm.png){fig-alt="Bar chart of top shooters (FT% > 75%, 3P% > 35%) by RAPM" width="85%"}

**Figure 10:** Top 20 elite shooters (FT% > 75%, 3P% > 35%) ranked by RAPM. Shooting skill correlates with but does not guarantee high impact—context and defense matter.

## Fixed Effects: Conferences and Seasons

### Conference Effects

```{r conference-effects-table}
conf_eff %>%
  arrange(desc(ridge_conf_per40)) %>%
  select(conference, ridge_conf_per40) %>%
  slice_head(n = 10) %>%
  bind_rows(
    conf_eff %>%
      arrange(ridge_conf_per40) %>%
      select(conference, ridge_conf_per40) %>%
      slice_head(n = 5)
  ) %>%
  kable(digits = 3,
        col.names = c("Conference", "Ridge Effect (per 40 min)"),
        caption = "Conference Fixed Effects: Top 10 and Bottom 5") %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE)
```

![Conference Effects Visualization](figs/conference_effects.png){fig-alt="Bar chart of conference fixed effects showing positive and negative impacts" width="85%"}

**Figure 11:** Conference fixed effects capture systematic over/under-performance relative to player talent. Positive effects may reflect superior coaching, systems, or development. Effects are modest (±2% WP per 40 min) but statistically meaningful.

### Season Effects

```{r season-effects-table}
season_eff %>%
  arrange(season) %>%
  select(season, ridge_season_per40) %>%
  kable(digits = 3,
        col.names = c("Season", "Ridge Effect (per 40 min)"),
        caption = "Season Fixed Effects (2018-2024)") %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE)
```

**Interpretation:** Season effects account for era-specific factors (rule changes, COVID-impacted 2020-21 season, overall competitiveness trends). The pattern shows slight variation across years, with more recent seasons closer to zero due to time-decay weighting.

## Sensitivity Analysis: Minutes Thresholds

```{r sensitivity-table}
tibble(
  Threshold = c("2000 min (main)", "1500 min", "2500 min"),
  N_Players = c(nrow(rapm_main), nrow(rapm_1500), nrow(rapm_2500))
) %>%
  kable(caption = "Sample Size by Minutes Threshold") %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE)
```

Overlap analysis (top 10 players):
- Main (2000 min) vs 1500 min: 10/10 agreement
- Main (2000 min) vs 2500 min: 6/10 agreement
- 1500 min vs 2500 min: 6/10 agreement

**Finding:** Rankings are stable for the very top players across thresholds, with some shuffling in positions 7-10 as sample requirements tighten.

# Limitations & Future Work

## Current Limitations

**Data constraints:**

1. **No substitution events:** ESPN data lacks sub-in/sub-out timestamps, forcing starter-based half-level approximations. We cannot track bench players systematically or capture within-half lineup changes.

2. **Starter bias:** Analysis limited to players who start games; bench specialists excluded from player-level RAPM (though they contribute to team-level estimates).

3. **Play context:** Limited defensive scheme, shot location, and play-type granularity available in hoopR data.

**Modeling assumptions:**

1. **Linear additivity:** Standard APM assumes player effects sum linearly; we don't model synergy/interaction effects (e.g., pick-and-roll chemistry).

2. **Position-agnostic shrinkage:** Ridge penalty treats all players equally; position-specific priors could improve estimates (guards may need different regularization than centers).

3. **Static effects:** Player RAPM is constant across seasons; we don't model growth curves or decline trajectories.

**Interpretation caveats:**

- Player RAPM is **exploratory** given starter-based stints; team-level APM more reliable
- Conference effects may partly reflect schedule strength not fully captured by opponent effects
- Extreme values at tails subject to high uncertainty despite regularization

## Recommended Next Steps

**Immediate priorities:**

1. **Acquire substitution data:** NCAA or other sources with official lineup data would enable possession-level stint tracking, transforming this from exploratory to definitive. Impact: Critical.

2. **Bayesian hierarchical priors:** Implement position/role-specific shrinkage (e.g., guards need less shrinkage for usage-heavy stats, centers need less for rim protection). Impact: Moderate improvement in small-sample estimates.

3. **Cross-validation robustness:** Test stability across seasons, conferences, and OOS leagues. Current validation is game-wise; season-wise CV would better test temporal stability. Impact: Validation of production-readiness.

**Longer-term extensions:**

1. **Opponent lineup interactions:** Model how player A performs vs specific opponent lineups or player types (e.g., stretch-5 vs traditional center). Requires possession-level data.

2. **Play-by-play decomposition:** Separate offensive and defensive impact using possession outcomes (shot quality, turnovers, rebounds). Requires richer event tagging.

3. **Uncertainty quantification:** Bootstrap or Bayesian MCMC for credible intervals on rankings. Current point estimates don't reflect estimation uncertainty.

4. **Real-time updating:** Build pipeline for in-season updates as new games complete. Current batch analysis; streaming updates would enable weekly rankings.

# Reproducibility Appendix

## Summary Statistics

```{r summary-stats-table}
rapm_stats %>%
  kable(digits = 5,
        col.names = c("N Players", "Mean RAPM", "Median RAPM", "SD RAPM", "Min RAPM", "Max RAPM"),
        caption = "RAPM Summary Statistics (≥2000 min, ≥50 games)") %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE)
```

## Top and Bottom Players

```{r top-bottom-table}
top_bottom %>%
  select(rank_type, player, ridge_rapm, games_played, avg_minutes) %>%
  kable(digits = 4,
        col.names = c("Category", "Player", "Ridge RAPM", "Games", "Avg Minutes"),
        caption = "Top 25 and Bottom 25 Players") %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE)
```

## Key Findings: Combined Visualization

![Combined Analysis Summary](figs/combined_analysis.png){fig-alt="Four-panel summary: top players, distribution, RAPM vs minutes, Ridge vs Lasso" width="100%"}

**Figure 12:** Four-panel summary visualization combining top players (A), distribution (B), RAPM vs minutes relationship (C), and regularization comparison (D).

## Computational Environment

```{r session-info, eval=FALSE}
# Session information saved during analysis
sessionInfo()
```

**Key packages:**

- `tidyverse` 2.0.0: Data manipulation and visualization
- `glmnet` 4.1-10: Regularized regression (ridge/lasso/elastic net)
- `Matrix` 1.7: Sparse matrix operations
- `hoopR` 2.1.0: ESPN data access
- `ranger` 0.16.0: Random forest implementation
- `xgboost` 1.7.8.1: Gradient boosting

**Hardware:** MacBook Pro M2, 9 cores utilized for parallel cross-validation

**Execution time:** Complete pipeline ~15-20 minutes (data download ~5 min, WP models ~3 min, RAPM estimation ~8 min, visualization ~2 min)

## Code to Reproduce Key Results

### Load and Inspect RAPM Results

```{r reproduce-rapm, eval=FALSE}
# Load main RAPM output
rapm_full <- readRDS("data/processed/rapm_results.rds")

# Extract components
rapm_table <- rapm_full$rapm_table  # Main results table
conference_fx <- rapm_full$conference_effects
season_fx <- rapm_full$season_effects
cv_ridge <- rapm_full$cv_ridge  # Cross-validation object

# Recompute shrinkage factor
baseline_sd <- sd(rapm_table$baseline_per40, na.rm = TRUE)
ridge_sd <- sd(rapm_table$ridge_per40, na.rm = TRUE)
shrinkage <- ridge_sd / baseline_sd
print(paste("Shrinkage factor:", round(shrinkage, 3)))

# Top players
rapm_table %>%
  arrange(desc(ridge_rapm)) %>%
  select(player, ridge_per40, games_played, total_minutes) %>%
  head(20)
```

### Recreate WP Model Comparison

```{r reproduce-wp, eval=FALSE}
# Load WP models
wp_full <- readRDS("data/interim/wp_models.rds")

# Best model name and calibration
best_model <- wp_full$best_model_name
eval_results <- wp_full$evaluation_results

# Test set performance for best model
eval_results %>%
  filter(Model == best_model, Dataset == "Test") %>%
  select(Model, AUC, Brier_Score, Calib_Intercept, Calib_Slope)

# Calibration curve data
calib_data <- wp_full$calibration_curves[[best_model]]
```

## Data Quality Checks

**Player ID validation (critical):**

```{r player-id-check, eval=FALSE}
# Check for name collisions
box_scores <- readRDS("data/interim/box_scores.rds")

name_collisions <- box_scores %>%
  distinct(player, player_id) %>%
  count(player) %>%
  filter(n > 1) %>%
  arrange(desc(n))

print(paste("Display names with multiple athlete IDs:", nrow(name_collisions)))
head(name_collisions, 10)
```

**Expected output:** ~1,411 collisions (e.g., "Jalen Johnson" → 18 different athletes)

---

**Analysis complete.** For questions or to request raw data access, contact the analysis team.

