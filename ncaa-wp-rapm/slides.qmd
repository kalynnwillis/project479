---
title: "Measuring What Matters"
subtitle: "Player Impact via Win Probability RAPM in NCAA Basketball"
author: "NCAA RAPM Analysis Team"
date: "October 6, 2025"
format:
  revealjs:
    theme: simple
    slide-number: true
    toc: false
    code-fold: false
    incremental: true
    preview-links: false
    width: 1600
    height: 900
    smaller: false
---

```{r setup, include=FALSE}
library(tidyverse)
library(readr)
library(knitr)

# Load key results
wp_eval <- read_csv("tables/wp_model_evaluation.csv", show_col_types = FALSE)
rapm_main <- read_csv("tables/rapm_rankings.csv", show_col_types = FALSE)
top_bottom <- read_csv("tables/top_bottom_players.csv", show_col_types = FALSE)
conf_eff <- read_csv("tables/conference_effects.csv", show_col_types = FALSE)

# Load full results objects
wp_models <- readRDS("data/interim/wp_models.rds")
rapm_results <- readRDS("data/processed/rapm_results.rds")

# Extract key metrics
best_model <- wp_models$best_model_name
best_calib_slope <- wp_eval %>% 
  filter(Model == best_model, Dataset == "Test") %>% 
  pull(Calib_Slope)
shrinkage_factor <- 0.199  # From logged output
```

# Background

## The Problem: Who Really Helps Teams Win? {.smaller}

**Scenario:** Two players, both average 15 points per game

::: {.fragment}
**Player A:**  
- Scores efficiently when team is winning  
- Makes mistakes in close games  
- Gets stats in blowouts  
:::

::: {.fragment}
**Player B:**  
- Scores critical baskets in clutch moments  
- Plays elite defense (not in box score)  
- Creates open shots for teammates  
:::

::: {.fragment}
**Question:** Who is more valuable? Traditional stats say they're equal!
:::

::: notes
Welcome. Imagine two NCAA players who both average 15 points per game. On paper, they look identical.

Player A is efficient but gets most of his points when the game is already decided—either his team is way ahead or way behind. In close games, he makes critical turnovers. He pads stats in garbage time.

Player B is the opposite. He hits clutch shots with the game on the line. He plays lockdown defense that doesn't show up in the box score. He makes smart passes that create easy buckets for teammates.

The question is: who's more valuable? Traditional statistics would say they're equal—both average 15 points. But intuitively, we know Player B helps his team win more. Our goal is to quantify this difference.
:::

---

## Why Traditional Metrics Fall Short {.smaller}

**Points Per Game (PPG):**  
- Doesn't account for *when* points are scored (clutch vs. garbage time)  
- Ignores defense, passing, spacing  
- Context-blind  

::: {.fragment}
**Plus-Minus (+/-):**  
- Mixes individual skill with teammate quality  
- "Good player on bad team" looks terrible  
- No adjustment for opponents  
:::

::: {.fragment}
**What We Need:** A metric that:  
✓ Weights **game context** (close games > blowouts)  
✓ **Isolates** individual impact from teammates  
✓ **Adjusts** for opponent strength  
:::

::: notes
Traditional metrics have three main problems.

Points per game treats all points equally. A basket to make it 90-70 counts the same as a basket to tie the game in the final minute. That makes no sense.

Plus-minus is better because it tracks team performance while a player is on court. But it has a fatal flaw: it mixes individual skill with context. A great player on a bad team will have a negative plus-minus just because his teammates are weak. We can't tell if the player is good or the team is good.

What we need is a metric that does three things: First, weight game context so close games matter more than blowouts. Second, isolate individual impact by accounting for teammates and opponents. Third, adjust for strength of competition.

That's what we built.
:::

---

## Our Solution: Win Probability Framework {.smaller}

**Key Insight:** Not all moments are equally important

::: {.fragment}
**Win Probability (WP):** Chance team will win given current game state  
- Score: 65-68, Time: 2:00 remaining → WP ≈ 30%  
- Score: 78-65, Time: 5:00 remaining → WP ≈ 95%  
:::

::: {.fragment}
**Our Metric:** Change in Win Probability (ΔWP) per minute  
- When Player X is on court, how much does WP change?  
- Automatically weights clutch situations  
- Directly measures impact on winning  
:::

::: notes
Our innovation is using a win probability framework.

Win probability is simple: at any moment in a game, what's the chance the home team wins? If you're down 3 points with 2 minutes left, maybe you have a 30% chance. If you're up 13 with 5 minutes left, maybe 95%.

Our metric tracks how this probability changes when specific players are on the court. If win probability goes from 50% to 60% during a 5-minute stretch, we want to know which players deserve credit.

This automatically handles context. A play that swings win probability from 50% to 60% is huge—that's a 10-point swing in a close game. A play that goes from 95% to 98% is basically meaningless—the game was already won.

We measure change in win probability per minute, adjust for teammates and opponents, and that gives us each player's true impact on winning.
:::

---

## Data & Limitations {.smaller}

**Dataset:** ESPN/hoopR, 7 seasons (2018-2024)  
- 38,625 games  
- 12.48 million plays  
- 35,457 unique players  

::: {.fragment}
**Critical Challenge:** No substitution data  
- Can't track exact lineup changes during games  
- Don't know when bench players enter/exit  
:::

::: {.fragment}
**Our Approach:**  
- Use **starter-based, half-level stints**  
- Starters play ~70-80% of minutes → captures most impact  
- Results are **exploratory** but methodologically sound  
:::

::: notes
We're using seven seasons of ESPN data—38,000 games and over 12 million individual plays. That's comprehensive coverage of NCAA Division 1 basketball.

But we have a critical limitation: ESPN doesn't provide substitution data. We don't know exactly when players check in and out. For NBA analysis, you'd have possession-by-possession lineup data. We don't have that luxury.

Our workaround is to use starter-based, half-level stints. We track the starting five for each team and assume they play the entire first half together, then the entire second half together. This isn't perfect, but starters typically play 70-80% of minutes, so we're capturing most of their impact.

This means our player-level results should be considered exploratory rather than definitive. But the methodology is sound, and with better data, this would be production-ready.
:::

# Analysis Overview

## Step 1: Build Win Probability Model {.smaller}

**Goal:** Predict P(home team wins) at any game moment

**Input Features:**  
- Score differential, time remaining  
- Possession, clutch indicator (time < 5 min, close game)  
- Interactions: score × time  

::: {.fragment}
**Models Tested:** Logistic, GBM, Random Forest, XGBoost

**Selection Criterion:** Calibration > Discrimination  
- **Calibration:** Do predicted probabilities match reality?  
- **Discrimination (AUC):** Can model rank outcomes?  
:::

::: {.fragment}
**Why Calibration Matters:** We use these probabilities to compute ΔWP  
- If model predicts 70% but truth is 60%, bias propagates through entire analysis  
:::

::: notes
Step 1 is building a win probability model. At any point in a game—say, 5 minutes left, home team down by 3 points—we want to predict the probability the home team wins.

We use features like score differential, time remaining, who has possession, and whether it's clutch time—defined as under 5 minutes in a close game. We also include interactions like score times time, because a 10-point lead with 1 minute left is different from 10 points with 10 minutes left.

We tested four model types: logistic regression, gradient boosting, random forest, and XGBoost. Most analysts would just pick the model with the highest AUC—the one that best ranks outcomes. But we did something different.

We prioritized calibration over discrimination. Calibration asks: do the predicted probabilities match reality? If you predict 70% win probability in 100 similar situations, do teams actually win 70 of those 100 games?

This matters because we're using these probabilities to compute win probability changes. If the model systematically overestimates or underestimates probabilities, that bias flows through our entire analysis. We can't afford that.
:::

---

## Model Selection Results {.smaller}

```{r wp-results, echo=FALSE}
wp_eval %>%
  filter(Dataset == "Test") %>%
  select(Model, AUC, Brier_Score, Calib_Slope) %>%
  mutate(Calib_Quality = case_when(
    abs(Calib_Slope - 1) < 0.2 ~ "Excellent",
    abs(Calib_Slope - 1) < 0.5 ~ "Good",
    TRUE ~ "Poor"
  )) %>%
  kable(digits = 3, col.names = c("Model", "AUC", "Brier Score", "Calib. Slope", "Quality"))
```

::: {.fragment}
**Winner:** Logistic Regression  
- Near-perfect calibration (slope = 1.05)  
- Slightly lower AUC than XGBoost, but much better probabilities  
- **Key lesson:** Simpler models can win when goal is probability estimation  
:::

::: notes
Here are our model results on test data. All four models achieve similar AUC scores around 0.85-0.86, meaning they all rank outcomes about equally well.

But look at calibration slope. Perfect calibration is a slope of 1.0. Logistic regression gets 1.05—nearly perfect. XGBoost gets 3.76—terrible. XGBoost is systematically overconfident. When it predicts 70%, the true probability might only be 55%.

We selected logistic regression. Despite having slightly lower AUC than XGBoost, its probabilities are trustworthy. This is a key lesson: the fanciest model isn't always the best model. When your goal is accurate probability estimation, sometimes the simple linear model wins.

This model forms the foundation for everything that follows. Every play in every game, we predict win probability. Then we track how it changes.
:::

---

## Win Probability Model Performance {.smaller}

![](figs/wp_calibration.png){fig-alt="Calibration curves for all models" width="80%"}

::: {.fragment}
- Logistic (purple) tracks diagonal = well-calibrated; XGBoost (yellow) overconfident  
- All models achieve similar AUC (~0.86), but calibration determines model selection  
:::

::: notes
This calibration plot shows why we chose logistic regression.

The diagonal line represents perfect calibration—predicted probabilities match observed frequencies. Logistic regression, in purple, tracks that diagonal beautifully. XGBoost in yellow systematically deviates, especially at extreme probabilities.

All four models achieve similar AUC scores around 0.85 to 0.86, meaning they rank outcomes about equally well. But calibration is what matters for our application. We're computing win probability changes, so we need accurate probabilities, not just good rankings.

This is why we selected the simpler logistic model over more complex alternatives.
:::

---

## Step 2: Construct Stints & Compute ΔWP {.smaller}

**Stints:** Half-level stretches with known starting lineups  
- First half: 5 home starters vs. 5 away starters (~20 minutes)  
- Second half: same process  
- Result: 37,974 valid 5-on-5 stints  

::: {.fragment}
**Compute ΔWP:**  
$$\Delta WP_{stint} = WP_{end} - WP_{start}$$
Normalize to per-minute scale, cap at ±2% per minute
:::

::: {.fragment}
**Weighting:**  
- **Duration:** Longer stints → more reliable (square root weighting)  
- **Leverage:** Close games (2.0×), blowouts (0.5×)  
- **No time decay:** All seasons weighted equally (historical evaluation, not forecasting)  
:::

::: notes
Step 2 is constructing stints and computing win probability changes.

Remember, we don't have substitution data, so we work with half-level stints. For each game's first half, we identify the 5 home starters and 5 away starters. We assume they play together the entire half. We repeat for the second half. This gives us about 38,000 valid 5-on-5 stints across seven seasons.

For each stint, we compute the change in win probability: final minus initial. We normalize this to a per-minute scale and cap extreme values at plus or minus 2% per minute to handle outliers.

Then we apply weights. Longer stints get more weight because they're more reliable—we use square root weighting to avoid over-emphasizing extremely long stints. Close games get double weight compared to normal games, and blowouts are down-weighted to half. 

Importantly, we do NOT apply time decay to down-weight older seasons. This is a historical evaluation across all seven seasons, not a forecasting exercise. We want to compare players fairly regardless of when they played. Season fixed effects in our model control for era-specific differences like rule changes.

This weighting is crucial. We want our estimates to reflect what matters: competitive game situations with reliable sample sizes.
:::

---

## Step 3: RAPM via Regularized Regression {.smaller}

**Problem:** Which players caused the ΔWP? Must account for teammates & opponents

**Design Matrix (sparse):**  
- Each row = one stint  
- Each column = one player  
- Home starters get +1, away starters get -1  
- **Fixed effects:** team, conference, season  

::: {.fragment}
**Regularization (Ridge, Lasso, Elastic Net):**  
- 10,000+ players, only 38,000 stints → severe overfitting risk  
- Shrinkage penalty pulls estimates toward zero  
- Players with limited data → conservative estimates  
- **Result:** Shrinkage factor = 0.20 (strong regularization)  
:::

::: notes
Step 3 is using regularized regression to isolate individual player effects—this is the RAPM part.

The problem is attribution. If a lineup's win probability goes up 10%, which of the 5 players deserve credit? And how do we account for the 5 opponents who were on the other side?

We build a design matrix where each row is a stint and each column is a player. If a player was a home starter, they get +1. If they were an away starter, -1. We also include fixed effects for teams, conferences, and seasons to control for systematic differences.

This is a huge matrix—10,000 players across 38,000 stints—and most entries are zero because only 10 players are on court each stint. That's why it's sparse.

Now here's the critical part: with 10,000 players and only 38,000 stints, we have severe overfitting risk. A player who happened to be on court for a few lucky stints could look like a superstar. That's where regularization comes in.

Ridge regression applies a shrinkage penalty that pulls all coefficients toward zero. Players with strong, consistent evidence of impact keep high estimates. Players with limited or noisy data get conservative estimates close to zero. Our final shrinkage factor is 0.20, meaning our estimates vary 20% as much as an unregularized model would produce. That's strong regularization, but it's necessary given the data noise.
:::

# Main Results

## Top Players: Impact on Winning {.smaller}

```{r top-players, echo=FALSE}
rapm_main %>%
  arrange(desc(ridge_rapm)) %>%
  select(player, ridge_per40, games_played, total_minutes) %>%
  slice_head(n = 8) %>%
  mutate(ridge_per40 = sprintf("%.3f", ridge_per40)) %>%
  kable(col.names = c("Player", "RAPM (per 40 min)", "Games", "Minutes"))
```

::: {.fragment}
**Interpretation:** Isiaih Mosley's 0.166 = **+16.6 percentage points of win probability**  
- In a tied game, his team starts with ~66.6% chance (instead of 50%)  
- That's 5 extra wins per 30-game season  
:::

::: notes
Here are our top players. We're showing their Ridge RAPM per 40 minutes—that's the win probability impact if they played a full game.

Isiaih Mosley from Missouri State leads at 0.166. Let me translate what this means in real terms.

In a 40-minute game, Mosley improves his team's win probability by 16.6 percentage points. Think about that: if two evenly matched teams play, the one with Mosley starts with a 66.6% chance of winning instead of 50%. That's a huge advantage.

Over a 30-game season, that 16.6-point edge translates to roughly 5 additional wins. In college basketball where tournaments decide everything, 5 extra wins can be the difference between making the NCAA tournament and missing it.

Notice the drop-off. The number 2 player, Sir'Jabari Rice, is at 0.143. By the time you get to the 5th or 6th player, you're around 0.09 or 0.10. Elite players really do stand out.

Also note these players have large sample sizes—100+ games, 3000+ minutes. We're not basing these rankings on small samples. These are well-established effects.
:::

---

## Top 30 Players Visualization {.smaller}

![](figs/top_players_rapm.png){fig-alt="Bar chart of top 30 players by Ridge RAPM" width="90%"}

::: {.fragment}
- Rapid drop-off after top 3-5 reflects appropriate shrinkage  
- All qualified players: ≥2000 minutes, ≥50 games  
:::

::: notes
This visualization shows the top 30 players ranked by Ridge RAPM. You can see the clear drop-off—Mosley at the top is significantly ahead of everyone else.

The gap between positions 1-3 and positions 4-10 is substantial. Then it flattens out. This is exactly what we want to see from our regularization. If there were no shrinkage, we'd see wild swings. If there were too much shrinkage, everyone would be at the same level.

This pattern reflects real differences in elite talent combined with conservative estimation for players with less overwhelming evidence. Remember, all these players cleared our minimum thresholds—at least 2000 total minutes and 50 games played.
:::

---

## Distribution: Most Players Are Average {.smaller}

::: {layout-ncol=2}
![](figs/rapm_distribution.png){fig-alt="Histogram showing normal distribution of RAPM values" width="100%"}

![](figs/ridge_vs_lasso.png){fig-alt="Ridge vs Lasso comparison" width="100%"}
:::

::: {.fragment}
- **Left:** Approximately normal, centered at zero—validates model assumptions
- **Right:** Ridge (20% shrinkage) preserves variation; Lasso eliminates most signal  
- **Trade-off:** Ridge measures all players conservatively vs Lasso's sparse selection
:::

::: notes
These plots show the full distribution of player impacts and compare regularization approaches.

The left histogram shows an approximately normal distribution centered near zero. This validates our modeling assumptions—most college players have neutral impact, with symmetric tails of positive and negative outliers. The bell shape confirms we're capturing real skill differences, not just noise.

The right plot compares Ridge and Lasso regression. Lasso is extremely aggressive—it pushes most coefficients all the way to zero, creating that horizontal line of dots. This achieves sparsity but eliminates signal.

Ridge regression applies moderate shrinkage—reducing standard deviation to 20% of unregularized estimates—but preserves the ranking and relative differences between all players.

We chose Ridge because our goal is measuring everyone, not just selecting a handful of stars. Ridge provides conservative but non-zero estimates for all 2,346 qualified players. Lasso would eliminate most of them.

This 20% shrinkage factor—down from 0.20 baseline to 0.04 ridge—is what prevents overfitting. Without it, random chance in stint-level data would create wildly unstable rankings.
:::

---

## Validation: No Minutes Bias {.smaller}

![](figs/rapm_vs_minutes.png){fig-alt="RAPM vs average minutes with loess smooth" width="70%"}

::: {.fragment}
- **Correlation: r = 0.136** (nearly flat)—regularization successfully removes minutes bias
- Unregularized models show r > 0.5 (volume bias)
- Proves we measure impact per minute, not playing time  
:::

::: notes
This is a critical validation check showing our regularization works correctly.

We're plotting RAPM against average minutes played per game. The correlation is r equals 0.136—nearly zero. This horizontal loess curve confirms we're measuring impact per minute, not volume.

Compare this to unregularized plus-minus, which shows strong positive correlation—r greater than 0.5—because good players accumulate both more minutes and better stats. That's volume bias.

Our near-zero correlation proves the regularization is working. High-minute players aren't systematically favored. A player averaging 25 minutes with high impact rates similar to one averaging 35 minutes with high impact, per minute of play.

This is exactly what we want from RAPM—measuring efficiency, not volume.
:::

---

## RAPM by Position {.smaller}

![](figs/rapm_by_position.png){fig-alt="Box plots of RAPM distribution by position" width="70%"}

::: {.fragment}
- **Guards (n=1,554):** Widest range (±0.005), 76% wider outlier spread than Centers
- **Centers (n=69):** Narrowest range (±0.003), tightest clustering—constrained roles limit variance
- **Key insight:** Ball-handling frequency drives impact differentiation  
:::

::: notes
Breaking down RAPM by position reveals distinct distributional patterns with precise quantitative differences.

Guards, with 1,554 players, show the widest range—plus or minus 0.005. They have 76% wider outlier spread than Centers. This makes sense—guards handle the ball the most, creating opportunities for both elite impact at plus 0.004 and value destruction at minus 0.005 through turnovers and bad shots.

Centers, with only 69 players, cluster most tightly—plus or minus 0.003. Their constrained roles—rim protection, rebounding, finishing—limit variance. While Centers actually have slightly higher standard deviation than Guards in absolute terms, Guards exhibit that 76% wider outlier range.

Forwards, with 690 players, show intermediate spread—plus or minus 0.003—reflecting their hybrid roles.

The key insight: ball-handling frequency drives impact differentiation. More touches mean more opportunity to demonstrate skill differences.

This suggests future work could use position-specific regularization, applying less shrinkage to high-variance positions like guards where there's more signal to extract.
:::

---

## Shooting Efficiency & RAPM {.smaller}

::: {layout-ncol=2}
![](figs/rapm_vs_shooting.png){fig-alt="RAPM vs free throw percentage, colored by 3P%" width="100%"}

![](figs/elite_shooters_rapm.png){fig-alt="Top elite shooters by RAPM" width="100%"}
:::

::: {.fragment}
- **Left:** Weak positive correlation (FT%: r = 0.05, 3P%: r = 0.10)—shooting explains <1% of RAPM variance
- **Right:** Elite shooters vary 3.2× in RAPM despite minimal shooting % differences  
- **Key insight:** 99% of RAPM variance comes from defense, playmaking, and situational performance—not shooting
:::

::: notes
These plots reveal a surprising finding about shooting skill and winning impact.

The left scatter plot shows RAPM against free throw percentage, colored by 3-point percentage. Yes, there's a positive correlation, but it's surprisingly weak: r equals 0.054 for free throw percentage and 0.095 for three-point percentage.

What does this mean? Shooting accuracy explains less than 1% of RAPM variance. Specifically, free throw percentage explains 0.3% and three-point percentage explains 0.9%. That means 99% of what makes players valuable comes from things other than shooting accuracy—defense, playmaking, basketball IQ, and situational performance.

Look at the massive vertical spread. Even among elite free throw shooters at 75-85%, RAPM ranges from -0.0025 to +0.0025. That's a huge range.

The right plot shows the top 20 elite shooters—at least 75% free throw shooting and 35% three-point shooting. Within this elite cohort, shooting accuracy varies minimally—just a 10 percentage point free throw range. But RAPM varies massively—a 3.2 times spread from 0.166 down to 0.052.

For example, Mosley leads at 0.166 RAPM despite having nearly identical shooting to the number 2 player—85.7% versus 85.2% free throw, 39.7% versus 39.2% three-point. The difference in impact isn't from shooting.

This is why comprehensive metrics like RAPM are essential. Shooting percentage tells you less than 1% of the story. The other 99% is invisible in box scores.
:::

---

## Conference Effects: Real but Secondary {.smaller}

![](figs/conference_effects.png){fig-alt="Conference fixed effects" width="60%"}

::: {.fragment}
**Findings:**  
- Conference effects are **statistically significant** (F = 22.4, p < 0.001)  
- But **practically modest:** ±4% WP per 40 minutes  
- **Player effects 4-5× larger:** Best player (+16.6%) vs. best conference (+3.7%)  
:::

::: {.fragment}
**Takeaway:** Individual talent >> conference affiliation  
:::

::: notes
This plot shows conference fixed effects—basically, how much better or worse teams perform than their roster quality would predict.

Big 12 teams outperform by 3.7 percentage points per 40 minutes. Non-major conferences underperform by about 5.0 points. The range is about 8.7 points total.

Now compare this to player effects. The range for players is approximately 37 percentage points—from Mosley at +16.6% to the worst players at around -20%. Players vary four to five times more than conferences.

An interesting statistical note: these conference effects are highly statistically significant. The F-test gives us F equals 22.4 with a p-value less than 0.001. That means the effect is real, not just random chance.

But statistical significance doesn't mean practical importance. With 38,000 stints, we have enormous statistical power to detect tiny effects. Adding conferences to our model only improves prediction error by 0.7%.

The takeaway is clear: individual talent dominates. Conference matters—probably through coaching, systems, and resources—but player skill is five times more important. If you're recruiting, you should care way more about getting great players than about which conference they come from.
:::

# Conclusion

## Key Results Summary {.smaller}

![](figs/combined_analysis.png){fig-alt="Four-panel summary: top players, distribution, RAPM vs minutes, Ridge vs Lasso" width="100%"}

::: notes
This four-panel summary captures our key findings visually.

Top left shows our top players—Mosley leading with 16.6 percentage points of win probability impact per 40 minutes. Clear drop-off from elite to very good.

Top right shows the distribution of all qualified players. Most cluster near zero, which is correct—most players are average. The approximately normal distribution validates our approach.

Bottom left is our validation check—RAPM versus average minutes. The flat relationship confirms we're not just measuring playing time. High-minute and low-minute players can have similar per-minute impact.

Bottom right compares our two regularization approaches. Lasso aggressively shrinks most players to exactly zero. Ridge preserves the ranking while pulling estimates toward zero proportionally. We chose Ridge for this application.

Together, these four plots demonstrate that our methodology produces sensible, validated, and interpretable results.
:::

---

## Summary & Future Directions {.smaller}

**What We Accomplished:**  
✓ Built well-calibrated win probability model (Logistic > XGBoost)  
✓ Quantified player impact: Top players have 12-17% WP impact per 40 min  
✓ Showed individual talent is 4-5× more important than conference  
✓ Validated methodology with proper regularization  

::: {.fragment}
**Next Steps:**  

1. **Acquire substitution data** → Possession-level analysis (transforms from exploratory to definitive)  

2. **Offensive/defensive decomposition** → Separate O-RAPM and D-RAPM to identify specialists  
  
:::

::: {.fragment}
**Applications:** Recruiting, transfer portal evaluation, lineup optimization
:::

::: notes
Let me summarize what we've accomplished and where this could go.

We built a win probability model that prioritizes calibration over pure discrimination. This was crucial—we showed that simpler models can outperform complex ones when accurate probabilities matter.

We quantified player impact on winning. The top players move the needle by 12-17 percentage points per 40 minutes. That's the difference between a 50-50 team and a team that wins 62-67% of their games.

We showed that while conferences have measurable effects, individual player talent is four to five times more important. This has implications for recruiting strategy.

And we validated our methodology. The strong regularization prevents overfitting. Our estimates are conservative but stable.

Looking forward, three priorities.

First, and most important, we need substitution data. If we knew exactly when players checked in and out, we could do possession-level analysis instead of half-level approximations. This would transform the analysis from exploratory to definitive. It would also let us include bench players systematically.

Second, we could decompose offense and defense. Right now, RAPM is a single number. But some players are elite defenders with limited offense. Others are offensive stars who struggle on defense. Separate O-RAPM and D-RAPM would identify these specialists.

Third, Bayesian hierarchical models with position-specific priors could provide smarter shrinkage. Right now, we treat all players the same. But guards and centers have different roles. Position-specific priors would improve estimates for players with limited data.

The practical applications are clear: recruiting, transfer portal evaluation, and lineup optimization. Teams could use this to identify undervalued players, make better roster decisions, and optimize rotations.

Thank you.
:::
