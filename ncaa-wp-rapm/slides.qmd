---
title: "Player Impact via Win Probability RAPM"
subtitle: "NCAA Men's Basketball Analysis (2018-2024)"
author: "NCAA RAPM Analysis Team"
date: "October 6, 2025"
format:
  revealjs:
    theme: simple
    slide-number: true
    toc: false
    code-fold: false
    incremental: true
    preview-links: false
    width: 1600
    height: 900
    smaller: false
---

```{r setup, include=FALSE}
library(tidyverse)
library(readr)
library(knitr)

# Load key results
wp_eval <- read_csv("tables/wp_model_evaluation.csv", show_col_types = FALSE)
rapm_main <- read_csv("tables/rapm_rankings.csv", show_col_types = FALSE)
top_bottom <- read_csv("tables/top_bottom_players.csv", show_col_types = FALSE)
conf_eff <- read_csv("tables/conference_effects.csv", show_col_types = FALSE)

# Load full results objects
wp_models <- readRDS("data/interim/wp_models.rds")
rapm_results <- readRDS("data/processed/rapm_results.rds")

# Extract key metrics
best_model <- wp_models$best_model_name
best_calib_slope <- wp_eval %>% 
  filter(Model == best_model, Dataset == "Test") %>% 
  pull(Calib_Slope)
shrinkage_factor <- 0.149  # From logged output
```

## Player Value Beyond the Box Score {.smaller}

**Goal:** Measure individual player impact on **win probability**, not just points

- Traditional stats (PPG, RPG) miss **context** and **team synergy**
- On/off metrics confound lineup quality with individual skill
- **Win Probability** captures game situation: score, time, leverage

::: {.fragment}
**Our Approach:** Regularized Adjusted Plus-Minus on the WP scale
:::

::: notes
Good afternoon. Today I'm presenting our analysis of player impact in NCAA men's basketball using a win probability framework. The goal is simple but powerful: we want to measure how much each player moves the needle on their team's chance of winning, not just how many points they score.

Traditional box score stats like points per game completely miss context. A bucket with 30 seconds left in a tie game is worth much more than one in garbage time. And simple on/off metrics get confounded by who you play with. Our approach uses Regularized Adjusted Plus-Minus on the win probability scale to isolate individual impact while accounting for game context.
:::

---

## The Data Challenge {.smaller}

**Data:** ESPN/hoopR, 7 seasons (2018-2024), 38K+ games, 12M+ plays

**Critical limitation:** No substitution events in ESPN data

::: {.fragment}
**Our solution:**
- Use **starter-based, half-level stints** (defensible proxy)
- Track unique players by `athlete_id` (critical: 1,411 name collisions!)
- Weight by duration × leverage (close/clutch games matter more)
- Time-decay older seasons (recent performance = more relevant)
:::

::: notes
We're working with seven seasons of ESPN data via the hoopR package—that's 38,000 games and over 12 million plays. But here's the challenge: ESPN doesn't provide substitution events. We don't know exactly when players check in and out.

Our solution is to use starter-based, half-level stints. It's not perfect, but it's defensible given the data constraints. And here's a critical detail we caught: we must track players by unique athlete ID, not display names. We found 1,411 display names that map to multiple different athletes—imagine aggregating 18 different "Jalen Johnsons" into one phantom super-player! That would completely invalidate our rankings.

We also weight stints by duration and leverage—close, clutch games get more weight than blowouts. And we apply time-decay so recent seasons matter more than 2018.
:::

---

## Two-Stage Modeling {.smaller}

**Stage 1: Win Probability Model**

Features: score diff, time, interactions, possession, clutch indicators

Models tested: Logistic, GBM, Random Forest, XGBoost

**Selection criteria:** Brier Score + Calibration (not just AUC!)

::: {.fragment}
**Stage 2: RAPM Estimation**

Response: ΔWP per minute (capped at ±2% for stability)

Design: Sparse matrix from player IDs + team/conference/season fixed effects

Regularization: Ridge/Lasso/Elastic + baseline OLS cross-check
:::

::: notes
Our approach has two stages. First, we build a win probability model. We tried four model types—logistic regression, gradient boosting, random forest, and XGBoost. But here's the key: we select the best model based on Brier score and calibration, not just AUC. Why? Because we're going to use these probabilities to compute win probability changes. If the model says 70% but the true rate is 60%, that bias propagates through our entire analysis.

Second stage: we estimate RAPM. The response variable is change in win probability per minute, capped at plus or minus 2% to handle outliers. We build a sparse design matrix from player IDs—remember, unique IDs are critical here—and include fixed effects for teams, conferences, and seasons. We use ridge, lasso, and elastic net regularization, plus an unregularized baseline as a sanity check.
:::

---

## Win Probability: Model Selection {.smaller}

```{r wp-results, echo=FALSE}
wp_eval %>%
  filter(Dataset == "Test") %>%
  select(Model, AUC, Brier_Score, Calib_Intercept, Calib_Slope) %>%
  kable(digits = 3, col.names = c("Model", "AUC", "Brier", "Calib. Intercept", "Calib. Slope"))
```

**Winner:** `r best_model` (Brier = `r sprintf("%.3f", wp_eval %>% filter(Model == best_model, Dataset == "Test") %>% pull(Brier_Score))`, Slope ≈ `r sprintf("%.2f", best_calib_slope)`)

Perfect calibration: Intercept ≈ 0, Slope ≈ 1

::: notes
Here are the results from our win probability model comparison. Looking at the test set, all models achieve AUC around 0.85 to 0.86—that's solid discrimination. But look at the calibration columns.

Perfect calibration means intercept near zero and slope near one. Logistic regression wins here with intercept of negative 0.23 and slope of 1.05—very close to perfect. XGBoost has the highest AUC but terrible calibration with a slope of 3.76. That would severely bias our win probability changes.

So we select Logistic regression. Sometimes the simple model wins when you care about getting probabilities right, not just rankings.
:::

---

## Win Probability: Calibration Quality {.smaller}

![Calibration curve for best model (Logistic): predicted vs observed win rate](figs/wp_calibration_best.png){fig-alt="Calibration plot showing logistic model predictions vs actual outcomes, with points near the diagonal indicating good calibration" width="75%"}

::: notes
This is the calibration plot for our winning model. The diagonal line is perfect calibration—predicted equals observed. The purple points show our Logistic model across different probability bins.

You can see the points hug that diagonal beautifully. When the model says 30% win probability, teams actually won about 30% of the time. When it says 80%, they won about 80% of the time. This is exactly what we need for accurate win probability change calculations.

The size of each point shows the number of observations in that bin—we have good coverage across the full probability range.
:::

---

## Model Performance Across Types {.smaller}

![Test AUC comparison across all models](figs/wp_model_performance.png){fig-alt="Bar chart comparing AUC scores across Logistic, GBM, Random Forest, and XGBoost models on train and test sets" width="75%"}

::: notes
This chart shows the AUC comparison across all four model types, with training AUC in light blue and test AUC in dark blue. All models show good generalization—test AUC is only slightly below training, indicating we're not overfitting.

XGBoost edges out slightly on AUC at 0.86, but remember, we chose Logistic because of calibration. This is a good reminder that the highest AUC doesn't always mean the best model for your use case. When you need well-calibrated probabilities, calibration matters more than a tiny AUC gain.
:::

---

## RAPM Results: Top Players {.smaller}

```{r top-players, echo=FALSE}
rapm_main %>%
  arrange(desc(ridge_rapm)) %>%
  select(player, ridge_per40, games_played, total_minutes) %>%
  slice_head(n = 10) %>%
  kable(digits = 3, col.names = c("Player", "Ridge RAPM (per 40 min)", "Games", "Minutes"))
```

**Key insight:** Each 0.001 RAPM ≈ 0.1% WP change per possession (× 40 min ≈ 80 possessions)

Isiaih Mosley: +0.131 per 40 min ≈ **+13% WP swing** in a full game

::: notes
Now for the payoff: player rankings. This table shows our top 10 players by Ridge RAPM, measured in win probability change per 40 minutes. These are players who meet our thresholds: at least 2,000 career minutes and 50 games across the seven seasons.

Isiaih Mosley tops the list at 0.131 per 40 minutes. To interpret this: in a typical 40-minute game with about 80 possessions, Mosley's impact is roughly a 13 percentage point swing in win probability compared to a replacement player. That's massive.

Sir'Jabari Rice is second, Gavin Kensmil third. Notice the shrinkage—these values are much more conservative than raw plus-minus would suggest. That's the regularization doing its job, pulling extreme estimates toward the mean based on sample size.
:::

---

## Distribution and Shrinkage {.smaller}

::: {layout-ncol=2}
![RAPM distribution across all qualified players](figs/rapm_distribution.png){fig-alt="Histogram of Ridge RAPM values showing normal distribution centered near zero" width="100%"}

![Ridge vs Lasso comparison](figs/ridge_vs_lasso.png){fig-alt="Scatter plot comparing Ridge and Lasso RAPM estimates" width="100%"}
:::

**Shrinkage factor:** `r sprintf("%.3f", shrinkage_factor)` (Ridge SD / Baseline SD)

Strong regularization → conservative, stable estimates

::: notes
The left plot shows the distribution of RAPM values across all qualified players. It's roughly normal and centered near zero, which is exactly what we expect. The majority of players cluster around average impact, with a few standout positive and negative outliers.

The right plot compares Ridge and Lasso. Lasso actually shrunk almost everything to zero—that's the horizontal line of points. Ridge preserved more variation. Our shrinkage factor is 0.149, meaning Ridge standard deviation is about 15% of the baseline unregularized estimates. That's strong shrinkage, which makes sense given the noise in stint-level data.

This conservative approach gives us more reliable rankings, especially for players with limited minutes.
:::

---

## Conference Fixed Effects {.smaller}

![Conference impact on win probability (adjusted for roster quality)](figs/conference_effects.png){fig-alt="Bar chart of conference fixed effects showing coaching/system impact beyond player talent" width="70%"}

**Interpretation:** Conferences with positive effects perform better than expected given roster talent (coaching, system, depth)

::: notes
This is one of the coolest findings. These are conference fixed effects from our model—essentially, how much better or worse a conference performs relative to what we'd expect from player talent alone.

Positive bars mean the conference overperforms its roster quality. This could reflect better coaching, system effects, or factors we're not capturing at the player level. Negative bars are underperformance.

The spread is meaningful but not huge, which suggests our player-level RAPM is capturing most of the variation. But conferences do matter—there's something about the Gonzaga, Duke, Kansas environments that squeezes extra wins beyond just having good players.
:::

---

## Key Findings {.smaller}

**Methodology validated:**
- Calibration-first WP model selection works (Logistic wins)
- Player ID tracking critical (prevented 1,411 name collisions!)
- Strong regularization essential for noisy stint data

**Substantive results:**
- Top players: Mosley, Rice, Kensmil (10-13% WP impact per 40 min)
- Conference effects modest but real (~±2% WP per 40 min)
- Shrinkage factor 0.15 → conservative, stable estimates

::: notes
Let me summarize the key takeaways. First, on methodology: calibration-first model selection was crucial—we avoided a biased XGBoost model. Player ID tracking prevented a disaster where we would have merged over 1,400 sets of different athletes. And strong regularization was essential given the noise in half-level stint data.

Substantively: we identified clear top performers with impact in the 10-13% win probability range per 40 minutes. Conference effects are real but modest—a couple percentage points—so player quality dominates. And our shrinkage factor of 0.15 gives us conservative estimates we can trust.
:::

---

## Next Steps {.smaller}

**Data improvements:**
1. **Substitution events** → possession-level lineups (would transform analysis)
2. Opponent lineup interactions → matchup-specific effects
3. Richer play context (shot location, defensive schemes)

**Modeling refinements:**
1. Bayesian hierarchical priors → smarter shrinkage by position/role
2. Cross-validation by season/conference → robustness checks
3. Uncertainty quantification → credible intervals for rankings

**Application:**
- Rotation optimization, transfer portal evaluation, recruiting
- Currently: **exploratory** (starter-based); with subs: **definitive**

::: notes
Finally, where do we go from here? The biggest opportunity is getting substitution data. If we knew exactly when players checked in and out, we could build possession-level lineups instead of half-level approximations. That would transform this from an exploratory analysis to a definitive player evaluation tool.

Beyond that, we could add opponent lineup interactions to capture matchup effects. More granular play context would help. On the modeling side, Bayesian hierarchical priors could give us smarter shrinkage that varies by position or role. And we should add uncertainty quantification—right now we have point estimates, but credible intervals would help with decisions.

In terms of application: rotation optimization, transfer portal evaluation, recruiting. But remember, the current results are exploratory because we're limited to starter-based stints. With substitution data, this becomes definitive.

Thank you! Questions?
:::

