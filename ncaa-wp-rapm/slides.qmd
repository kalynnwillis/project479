---
title: "Player Impact via Win Probability RAPM"
subtitle: "NCAA Men's Basketball Analysis (2018-2024)"
author: "NCAA RAPM Analysis Team"
date: "October 6, 2025"
format:
  revealjs:
    theme: simple
    slide-number: true
    toc: false
    code-fold: false
    incremental: true
    preview-links: false
    width: 1600
    height: 900
    smaller: false
---

```{r setup, include=FALSE}
library(tidyverse)
library(readr)
library(knitr)

# Load key results
wp_eval <- read_csv("tables/wp_model_evaluation.csv", show_col_types = FALSE)
rapm_main <- read_csv("tables/rapm_rankings.csv", show_col_types = FALSE)
top_bottom <- read_csv("tables/top_bottom_players.csv", show_col_types = FALSE)
conf_eff <- read_csv("tables/conference_effects.csv", show_col_types = FALSE)

# Load full results objects
wp_models <- readRDS("data/interim/wp_models.rds")
rapm_results <- readRDS("data/processed/rapm_results.rds")

# Extract key metrics
best_model <- wp_models$best_model_name
best_calib_slope <- wp_eval %>% 
  filter(Model == best_model, Dataset == "Test") %>% 
  pull(Calib_Slope)
shrinkage_factor <- 0.149  # From logged output
```

## Player Value Beyond the Box Score (Background) {.smaller}
**Goal:** Measure individual player impact on **win probability**, not just points

- Traditional stats (PPG, RPG) miss **context** and **team dynamics**
- On/off the court metrics mix the effects of lineup quality with individual skill
- **Win Probability** captures game status: score, time, leverage

::: {.fragment}
**Our Approach:** Regularized Adjusted Plus-Minus on the WP scale
:::

::: notes
Good afternoon. We are presenting our analysis of player impact in NCAA D1 men's basketball using a win probability framework. The goal is simple but powerful: we want to measure how much each player alters on their team's chance of winning, not just how many points they score.

Traditional box score statistics, such as points per game, do not include the context. A bucket with 30 seconds left in a tie game is worth much more than one in non-clutch time. The on/off metrics get mixed up by who is playing with who. Our approach uses Regularized Adjusted Plus-Minus on the win probability scale to isolate individual impact while accounting for game context.
:::

---

## The Data Challenge (Background) {.smaller}
#Background
**Data:** ESPN/hoopR, 7 seasons (2018-2024), 38K+ games, 12M+ plays

**Limitation:** No substitution events in the data (tracking players on the court)

::: {.fragment}
**Our solution:**
- Use **starter-based, half-level stints** (justifiable)
- Track unique players by `athlete_id` (1,411 name collisions)
- Weight: duration × leverage (close/clutch games matter)
:::

::: notes
We're working with seven seasons of ESPN data via the hoopR package—that's 38,000 games and over 12 million plays. But our challenge is that ESPN doesn't provide substitution events, so we do not know exactly when players sub in and out for on-court lineups. Thus, granularly, we do not know the exact moments players are on the court.

- Our solution is to use starter-based, half-level stints because this gives a clean window to evaluate performance. Therefore, we know that the starters are on the court at the start of the half until the first sub or timeout. It's not perfect, but it's justifiable given the data constraints. 
- We then found that we must track players by unique athlete ID, not display names, because we found 1,411 names that map one to many with multiple different athletes—imagine aggregating 18 different "Jalen Johnsons" into one phantom super-player! That would completely invalidate our rankings.
- We then weighted the stints by duration and leverage—close, clutch games get more weight than blowouts. 
- And we apply time decay, so recent seasons matter more than 2018.

:::

---

## Modeling Steps (Analysis) {.smaller}
#Analysis
**Part 1: Win Probability Model**

Features: score differential, time, interactions, possession, clutch-time indicators

Models tested: Logistic, GBM, Random Forest, XGBoost

**Selection by:** Brier Score + Calibration (not just AUC!)

::: {.fragment}
**Part 2: RAPM Estimation**

Response: ΔWP per minute (limit: +/-2%)

Design Matrix: player IDs + unique IDs + fixed effects for team/conference/season

Regularization: Ridge/Lasso/Elastic + baseline OLS
:::

::: notes
Our approach has two parts. First, we build a win probability model that predicts the probability a team will win at any moment in the game. We tried four model types—logistic regression, gradient boosting, random forest, and XGBoost. Rather than just looking at how well the models rank outcomes using AUC, we considered the Brier score and calibration to look at the accuracy. We wanted the predicted probabilities to be as close as possible to the real probabilities of winning. For example, if the model predicts a 70% chance of winning, but in reality it is only 60%. This difference causes problems later in the second part with the accuracy of our calculations, as they rely on these probabilities.

In part 2, we estimate RAPM. The response variable is the change in win probability per minute, restricted at plus or minus 2% which handles outliers. We build a model matrix from player IDs, unique IDs, and the fixed effects for teams, conferences, and seasons. Our matrix has most of the values at zero because only 5 players are active on the court at one time. The unique player IDs are critical because there are several names that map to multiple different athletes. The fixed effects help isolate player skill by controlling for variations in team strength, season chances, etc.. Then, to regularize, we use ridge, lasso, and elastic net regularization, and an unregularized baseline to prevent overfitting. The unregularized baseline is a great sanity check to compare results with the regularized methods to ensure they worked.

In the next step, we create a design matrix to represent which players are on the court during each stint of the game. This matrix is sparse, meaning most of the values are zero because only 5 players are active at any time. Using unique player IDs is very important here to correctly track each individual’s contribution.
We also include fixed effects for teams, conferences, and seasons. These fixed effects help control for differences in team strength, the competitive level of different conferences, and changes across seasons, so we can better isolate each player’s true impact.

To estimate player effects, we apply several types of regularized regression models: ridge, lasso, and elastic net. These techniques help prevent overfitting by adding a shrinkage penalty to bring coefficients toward zero to make the model more stable, as one player's effect does not take all the credit. We also run an unregularized baseline model without any penalty as a sanity check, to compare results and ensure our regularization methods are working as expected.

:::

---

## Model Selection (Analysis) {.smaller}

```{r wp-results, echo=FALSE}
wp_eval %>%
  filter(Dataset == "Test") %>%
  select(Model, AUC, Brier_Score, Calib_Intercept, Calib_Slope) %>%
  kable(digits = 3, col.names = c("Model", "AUC", "Brier", "Calib. Intercept", "Calib. Slope"))
```

**Winner:** `r best_model` (Brier = `r sprintf("%.3f", wp_eval %>% filter(Model == best_model, Dataset == "Test") %>% pull(Brier_Score))`, Slope ≈ `r sprintf("%.2f", best_calib_slope)`)

Perfect calibration: Intercept ≈ 0, Slope ≈ 1

::: notes
Here are the results from our win probability model comparison. Looking at the test data, all models have AUC scores around 0.85 to 0.86, which means they rank the team's chances of winning appropriately. 

However, when we look at calibration (accuracy of probabilities), the differences are apparent. Perfect calibrations have an intercept near zero and a slope near one. Here, the logistic regression model has an intercept of -0.23 and a slope of 1.05, which is close to perfect calibration. XGBoost has the highest AUC, yet has bad calibration as the slope is 3.76, which means the predicted win probabilities would be unreliable and inaccurate.

Therefore, we selected the logistic regression model. Sometimes the simple model wins when the goal is to get accurate probabilities, rather than adequate rankings.
:::

---

## Win Probability: Calibration Quality (Analysis) {.smaller}

![Calibration curve for best model (Logistic): predicted vs observed win rate](figs/wp_calibration_best.png){fig-alt="Calibration plot showing logistic model predictions vs actual outcomes, with points near the diagonal indicating good calibration" width="75%"}

::: notes
This chart shows how well our selected model predicted probabilities against what was observed and happened. The diagonal line is where the predicted outcomes are equal to the observed outcomes. The points on the plot show our model across various probabilities, which line up very close to the baseline diagonal line. For example, it predicts a 30% chance of winning when the teams really do win 30% of the time.
The size of each point shows how many plays are in that range. This model shows that our data covers various win probabilities from low to high.
:::

---

## Model Performance Across Types (Analysis) {.smaller}

![Test AUC comparison across all models](figs/wp_model_performance.png){fig-alt="Bar chart comparing AUC scores across Logistic, GBM, Random Forest, and XGBoost models on train and test sets" width="75%"}

::: notes
This chart compares the AUC scores across all four models. The light blue shows training AUC, and the dark blue is the test AUC. The models show that AUC is only slightly lower than the training ones, which shows that overfitting is not apparent in our model on the training data, but focuses more on the actual relationships and patterns.

XGBoost has a higher AUC at 0.86, but we selected the logistic regression model because it is calibrated better. Higher AUC doesn't always mean it is the best because better calibration means more accurate probabilities, which is more valuable than slightly better rankings.
:::

---

## RAPM Results: Top Players (Results) {.smaller}

```{r top-players, echo=FALSE}
rapm_main %>%
  arrange(desc(ridge_rapm)) %>%
  select(player, ridge_per40, games_played, total_minutes) %>%
  slice_head(n = 10) %>%
  kable(digits = 3, col.names = c("Player", "Ridge RAPM (per 40 min)", "Games", "Minutes"))
```

**Key insight:** Each +0.001 RAPM ≈ +0.1% WP change per possession (40 min ≈ 80 possessions)

Isiaih Mosley: +0.131 per 40 minutes ≈ **+13% Win Probability swing** in a full 40-minute game

::: notes
Now, for player rankings, this table displays the top 10 players based on their Ridge RAPM, which means how much a player changes their team's win probability for every 40 minutes on the court. It is important to note that we only include players who have at least 2,000 career minutes and over 50 games throughout 7 seasons. We picked these arbitrary thresholds because we wanted to ensure there was enough data for players, and due to our data limitations based on the lack of substitution data, we may focus more on starters. 

Isiaih Mosely is at the top of the list with a RAPM of 0.131. This means for every 40 minutes, a typical 40-minute game, with about 80 possessions, his presence impacts the team's win probability about 12 percentage points compared to a relative, baseline replacement player, which is a lot!

Sir'Jabari Rice and Gavin Kensmil do not have large values as they are more realistic and have a shrinkage penalty that brings their effect closer to baseline 0.  
:::

---

## Distribution and Shrinkage (Results) {.smaller}

::: {layout-ncol=2}
![RAPM distribution across all qualified players](figs/rapm_distribution.png){fig-alt="Histogram of Ridge RAPM values showing normal distribution centered near zero" width="100%"}

![Ridge vs Lasso comparison](figs/ridge_vs_lasso.png){fig-alt="Scatter plot comparing Ridge and Lasso RAPM estimates" width="100%"}
:::

**Shrinkage factor:** `r sprintf("%.3f", shrinkage_factor)` (Ridge SD / Baseline SD)

Strong regularization (moderate to large shrinkage penalty) → conservative and consistent estimates

::: notes
The left plot shows the distribution of RAPM values for all qualified players that meet our threshold requirements. This appears to be approximately normal and centered around zero, which lines up with our expectations because most players have an average impact, as some stand out or have a negative impact, with the exception that some stand-out players are outliers. 

The plot on the right compares the Ridge and Lasso estimates. Lasso pushes almost everything to zero on this line of points. Then, the Ridge maintains the variation across players with a shrinkage factor of 0.149, which means estimates vary 15% as much as the unregularized estimates. This is a strong shrinkage factor, but there may be a lot of random noise in the data for every stint.

This regularization is more conservative and has smaller impact estimates because we do not want a model to be mostly guided by noise, and it prevents players with less data from being unfairly estimated as good or bad.
:::

---

## Conference Fixed Effects (Results) {.smaller}

![Conference impact on win probability (adjusted for roster quality)](figs/conference_effects.png){fig-alt="Bar chart of conference fixed effects showing coaching/system impact beyond player talent" width="70%"}

**Key finding:** Conference effects are **statistically significant but practically modest**

- Conference range: +/-3% WP per 40 minutes
    - Big Ten +2.6% & Other -3.4%
- Player range: +/-15% WP per 40 minutes 
- **Player effects 5× larger than conference effects** → individual talent dominates
- Statistical test: F = 22.4 (p < 0.001), but only 0.7% Mean Squared Error improvement

::: notes
These are interesting findings, with one being that these numbers are conference fixed effects from the model. This shows how much better or worse a conference team's performance is based on the talent of the players. 

The range of conference effects is 6 percentage points of win probability per 40 minutes, as the Big Ten is about +2.6% and the other conferences are at -3.4%. Individual player effects are bigger by around 30 percentage points. Isiaih Mosely is +13% and the worst players are at -15%. 

Conferences are statistically significant because when we compare models with and without fixed effects, the F-statistic is 22.4 with a p-value less than .0001, which means the effect is highly unlikely due to random chance. However, we found that adding the conferences improved prediction error by reducing the Mean Squared Error by 0.7%, which is small, whereas player effects are 5 times larger. 

Since we have 37,668 stints, which is a huge sample, this large amount of data can create small differences to show up as real effects, rather than noise. However, logistically speaking, the effect is still very small compared to individual players' skills.

Thus, within the context of how much NCAA D1 basketball players impact their team's success is that while conferences have effects on win success, like probably by having specific resources, systems, and coaching, the player's individual talent is what impacts the success outcome more. 
:::

---

## Key Findings (Conclusion) {.smaller}

**Methodology validated:**
- Calibration to selection models → Logistic regression model
- Player ID tracking (prevented 1,411 player merges!)
- Strong regularization to combat noisy stint-level data

**Substantive results:**
- Top players: Isiaih Mosley, Sir'Jabari Rice, and Gavin Kensmil (10-13% WP impact per 40 minutes)
- **Player effects: 5× bigger than conference effects** (individual talent > conference)
- Conference effects: apparent (+2.6% Big Ten, -3.4% Other) but not relatively impactful
- Shrinkage factor 0.15 → conservative, stable estimates

::: notes
In summary, we chose models based on calibration to prevent choosing models that rank the players well but do not adequately estimate their win probabilities. 

It was helpful to track player IDs so >1,400 athletes were not merged.
The strong regularization with the large shrinkage penalty of 0.15 was essential to handle the noise in our data to create more conservative estimates, especially with players who had fewer minutes.

Top players have 10-13% impact on win probability per 40 minutes, which is well above the baseline of 0% impact. While conference effects are apparent, they are not substantial, it is only by 0.7% percentage points, which signifies player's individual talent is what drives a team's success more.
:::

---

## Next Steps (Conclusion) {.smaller}

**Data improvements:**
1. **Substitution events** → possession-level data
2. Opponent lineup interactions → player-matchup effects
3. Play context (shot location, defensive schemes)

**Modeling refinements:**
1. Bayesian hierarchical priors → smarter shrinkage by position/role and better intervals for ranking
2. Cross-validation by season or conference → estimate out-of-sample error → evaluate model robustness

**Application:**
- Efficient lineups, transfer portal, recruiting

::: notes
The biggest opportunity is getting substitution data. If we knew exactly when players checked in and out onto the court, we could build possession-level lineup data rather than half-level approximations and be able to include more data on players who are not starters.

We could add opponent lineup interactions to obtain player-matchup effects. All in all, if we have more granular play data with more context, we can create improved estimates. 

Bayesian hierarchical priors can give smarter shrinkage that can vary by position, which includes uncertainty quantification like credible intervals rather than point estimates.

This model/tool can then be applied to help with evaluating players in the transfer portal, recruiting between schools, and building efficient lineups. 

Thank you for your time!
:::

